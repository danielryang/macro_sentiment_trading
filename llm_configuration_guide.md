# LLM Configuration Guide Complete guide for configuring LLM-enhanced sentiment analysis through your `.env` file. ## [TOOL] **Quick Start** ### **1. Basic Configuration (Free)** Add to your `.env` file: ```bash # Use local models only (no API costs) LLM_USE_OPENAI=false LLM_USE_ANTHROPIC=false LLM_USE_LOCAL=true LLM_FALLBACK_FINBERT=true ``` ### **2. Enhanced Configuration (With APIs)** Add to your `.env` file: ```bash # Enable API providers LLM_USE_OPENAI=true LLM_USE_ANTHROPIC=true LLM_USE_LOCAL=true LLM_FALLBACK_FINBERT=true # Add your API keys OPENAI_API_KEY=sk-your-openai-key-here ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here # Choose cost-effective models LLM_OPENAI_MODEL=gpt-4o-mini LLM_ANTHROPIC_MODEL=claude-3-haiku-20240307 ``` ## **Complete Configuration Options** ### **API Provider Settings** ```bash # Enable/Disable each provider LLM_USE_OPENAI=true/false # OpenAI GPT models LLM_USE_ANTHROPIC=true/false # Anthropic Claude models LLM_USE_LOCAL=true/false # Local transformer models LLM_FALLBACK_FINBERT=true/false # FinBERT fallback (always recommended) ``` ### **API Keys** ```bash # OpenAI API Key (get from https://platform.openai.com/api-keys) OPENAI_API_KEY=sk-your-openai-key-here # Anthropic API Key (get from https://console.anthropic.com/) ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here ``` ### **Model Selection** #### **OpenAI Models:** ```bash LLM_OPENAI_MODEL=gpt-4o-mini # Recommended: Fast, cheap (~$0.15/1M tokens) # LLM_OPENAI_MODEL=gpt-3.5-turbo # Budget option (~$1.50/1M tokens) # LLM_OPENAI_MODEL=gpt-4o # Balanced (~$5.00/1M tokens) # LLM_OPENAI_MODEL=gpt-4 # Premium (~$30.00/1M tokens) ``` #### **Anthropic Models:** ```bash LLM_ANTHROPIC_MODEL=claude-3-haiku-20240307 # Recommended: Fast, cheapest (~$0.25/1M tokens) # LLM_ANTHROPIC_MODEL=claude-3-sonnet-20240229 # Balanced (~$3.00/1M tokens) # LLM_ANTHROPIC_MODEL=claude-3-opus-20240229 # Premium (~$15.00/1M tokens) ``` ### **Performance Settings** ```bash LLM_BATCH_SIZE=8 # How many headlines to process at once LLM_CONFIDENCE_THRESHOLD=0.7 # Minimum confidence for predictions LLM_CACHE_DIR=data/cache/llm_sentiment # Where to cache results ``` ### **Rate Limiting & Cost Controls** ```bash LLM_MAX_REQUESTS_PER_MINUTE=60 # API rate limiting LLM_REQUEST_TIMEOUT=30 # Timeout in seconds LLM_MAX_TOKENS=200 # Max response length (controls cost) LLM_TEMPERATURE=0.1 # Lower = more consistent (0.0-2.0) ``` ## [CHART] **Model Recommendations** ### **Cost-Effective Setup (~$2/month)** ```bash LLM_USE_OPENAI=true LLM_USE_ANTHROPIC=true LLM_OPENAI_MODEL=gpt-4o-mini LLM_ANTHROPIC_MODEL=claude-3-haiku-20240307 LLM_MAX_TOKENS=150 # Reduce for lower costs ``` ### **High Performance Setup (~$10/month)** ```bash LLM_USE_OPENAI=true LLM_USE_ANTHROPIC=true LLM_OPENAI_MODEL=gpt-4o LLM_ANTHROPIC_MODEL=claude-3-sonnet-20240229 LLM_MAX_TOKENS=200 ``` ### **Free Setup (No API costs)** ```bash LLM_USE_OPENAI=false LLM_USE_ANTHROPIC=false LLM_USE_LOCAL=true LLM_FALLBACK_FINBERT=true ``` ## [LAUNCH] **Usage Examples** ### **Automatic Configuration (Recommended)** ```python from src.llm_sentiment_analyzer import LLMSentimentAnalyzer # Automatically loads configuration from .env analyzer = LLMSentimentAnalyzer() headlines = ["Fed raises rates to combat inflation"] results = analyzer.analyze_sentiment_batch(headlines) ``` ### **Override Configuration** ```python # Override .env settings with parameters analyzer = LLMSentimentAnalyzer( use_openai=True, # Override .env setting openai_model="gpt-4", # Override model choice temperature=0.2 # Not supported yet in overrides ) ``` ### **Custom Configuration Object** ```python from src.llm_sentiment_analyzer import LLMSentimentConfig, LLMSentimentAnalyzer # Create custom config config = LLMSentimentConfig() config.use_openai = True config.openai_model = "gpt-4" analyzer = LLMSentimentAnalyzer(config=config) ``` ## [SEARCH] **Testing Your Configuration** Run this to test your setup: ```bash cd macro_sentiment_trading python src/llm_sentiment_analyzer.py ``` Expected output with APIs configured: ``` [TEST 1] Testing with .env configuration... INFO:llm_sentiment_analyzer:LLM Config loaded: OpenAI=True(gpt-4o-mini), Anthropic=True(claude-3-haiku-20240307), Local=True, FinBERT=True INFO:llm_sentiment_analyzer:OpenAI client initialized successfully INFO:llm_sentiment_analyzer:Anthropic client initialized successfully Testing .env API-based analysis... API Result: {'negative': 0.15, 'neutral': 0.25, 'positive': 0.60, 'polarity': 0.45} ``` ## [TARGET] **Configuration Strategies** ### **Development Strategy** ```bash # Start with free, add APIs gradually LLM_USE_OPENAI=false LLM_USE_ANTHROPIC=false LLM_USE_LOCAL=true LLM_FALLBACK_FINBERT=true ``` ### **Production Strategy** ```bash # Reliable multi-provider setup LLM_USE_OPENAI=true LLM_USE_ANTHROPIC=true LLM_USE_LOCAL=true LLM_FALLBACK_FINBERT=true # Cost-effective models LLM_OPENAI_MODEL=gpt-4o-mini LLM_ANTHROPIC_MODEL=claude-3-haiku-20240307 # Conservative rate limits LLM_MAX_REQUESTS_PER_MINUTE=30 LLM_MAX_TOKENS=150 ``` ### **Research Strategy** ```bash # Maximum accuracy for research LLM_USE_OPENAI=true LLM_USE_ANTHROPIC=true LLM_OPENAI_MODEL=gpt-4 LLM_ANTHROPIC_MODEL=claude-3-opus-20240229 LLM_MAX_TOKENS=300 LLM_TEMPERATURE=0.0 # Maximum consistency ``` ## **Security & Best Practices** ### **API Key Security** 1. **Never commit API keys** to version control 2. **Use `.env` file** (add to `.gitignore`) 3. **Set usage limits** in API dashboards 4. **Monitor costs** regularly ### **Cost Control** 1. **Start with cheap models** (haiku, gpt-4o-mini) 2. **Use low max_tokens** (150-200) 3. **Enable caching** (default enabled) 4. **Set rate limits** in .env file ### **Performance Optimization** 1. **Enable multiple providers** for redundancy 2. **Use local models** as backup 3. **Always enable FinBERT fallback** 4. **Adjust batch size** based on usage ## **Troubleshooting** ### **API Not Working** 1. Check API key format and validity 2. Verify model name spelling 3. Check account balance/limits 4. Test with simple example first ### **High Costs** 1. Reduce `LLM_MAX_TOKENS` 2. Use cheaper models 3. Check `LLM_BATCH_SIZE` setting 4. Monitor API dashboard ### **Configuration Not Loading** 1. Ensure `.env` file is in project root 2. Check variable spelling 3. Restart application after changes 4. Verify `python-dotenv` is installed ## **Ready to Use!** Your system will automatically: 1. **Load configuration** from `.env` file 2. **Initialize available APIs** based on keys 3. **Fallback gracefully** if APIs fail 4. **Cache results** to minimize costs 5. **Provide detailed logging** for monitoring **Start with the free configuration and upgrade as needed!**