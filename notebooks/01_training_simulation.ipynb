{ "cells": [ { "cell_type": "markdown", "id": "11eb2560", "metadata": {}, "source": [ "# Macro Sentiment Trading - Complete Lifecycle\n", "\n", "This notebook demonstrates the complete pipeline from data collection to signal generation:\n", "1. **Data Collection** - Collect GDELT news data (6 months)\n", "2. **Sentiment Processing** - Analyze sentiment and create features\n", "3. **Market Data Processing** - Align market data with sentiment\n", "4. **Model Training** - Train ML models on aligned data\n", "5. **Backtesting** - Test model performance\n", "6. **Signal Generation** - Generate current trading signals\n" ] }, { "cell_type": "markdown", "id": "203f1d45", "metadata": {}, "source": [ "## Setup and Imports\n" ] }, { "cell_type": "code", "execution_count": 1, "id": "43de0e1d", "metadata": {}, "outputs": [ { "name": "stdout", "output_type": "stream", "text": [ "Project root: c:\\Users\\danie\\Coding Projects\\Personal\\macro_sentiment_trading\n", "Data directory: c:\\Users\\danie\\Coding Projects\\Personal\\macro_sentiment_trading\\data\n", "Results directory: c:\\Users\\danie\\Coding Projects\\Personal\\macro_sentiment_trading\\results\n" ] } ], "source": [ "# Core imports\n", "import pandas as pd\n", "import numpy as np\n", "import logging\n", "from datetime import datetime, timedelta\n", "from pathlib import Path\n", "import warnings\n", "warnings.filterwarnings('ignore')\n", "\n", "# Configure logging\n", "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n", "logger = logging.getLogger(__name__)\n", "\n", "# Set up paths\n", "PROJECT_ROOT = Path.cwd().parent\n", "DATA_DIR = PROJECT_ROOT / \"data\"\n", "RESULTS_DIR = PROJECT_ROOT / \"results\"\n", "\n", "print(f\"Project root: {PROJECT_ROOT}\")\n", "print(f\"Data directory: {DATA_DIR}\")\n", "print(f\"Results directory: {RESULTS_DIR}\")\n" ] }, { "cell_type": "markdown", "id": "429540d5", "metadata": {}, "source": [ "## Step 1: Data Collection (6 Months)\n", "\n", "Collect GDELT news data using BigQuery with built-in cost controls.\n" ] }, { "cell_type": "code", "execution_count": 2, "id": "ec86b5f7", "metadata": {}, "outputs": [ { "name": "stderr", "output_type": "stream", "text": [ "2025-10-23 02:11:51,589 - INFO - [CACHE] Loading primary cached data from: data\\news\\gdelt_bigquery_2024-01-01_2024-06-30.parquet\n" ] }, { "name": "stdout", "output_type": "stream", "text": [ "Collecting news data from 2024-01-01 to 2024-06-30\n", "Using BigQuery with built-in cost controls...\n" ] }, { "name": "stderr", "output_type": "stream", "text": [ "2025-10-23 02:11:53,714 - INFO - [OK] Loaded 18200 cached events (3.7 MB)\n", "2025-10-23 02:11:53,731 - INFO - [DATE] Date range: 2024-06-30 00:00:00 to 2024-06-30 00:00:00\n" ] }, { "name": "stdout", "output_type": "stream", "text": [ "\n", "Data collection completed!\n", "Shape: (18200, 13)\n", "Columns: ['date', 'full_date', 'headline', 'url', 'tone', 'doc_id', 'goldstein_mean', 'goldstein_std', 'num_articles', 'num_mentions', 'num_sources', 'actor1_count', 'actor2_count']\n", "Date range: 2024-06-30 00:00:00 to 2024-06-30 00:00:00\n", "\n", "Sample headlines:\n", "1. Canicula revine &#xEE;n Rom&#xE2;nia. Aproape toat&#x103; &#x21B;ara este sub cod galben, doar c&#xE2;teva jude&#x21B;e nu sunt afectate - Vremea noua\n", "2. Arcidosso, da luned&#xEC; 1&#xB0; luglio operativa la nuova biglietteria At\n", "3. Revolt&#x103; &#xEE;n Germania. O femeie a primit o sentin&#x21B;&#x103; mai dur&#x103; dec&#xE2;t un t&#xE2;n&#x103;r condamnat pentru viol, pentru c&#x103; l-a insultat - Vremea noua\n", "4. &#x7965;&#x9E4F;&#x822A;&#x7A7A;&#x5F00;&#x5C55;&#x6D88;&#x9632;&#x4F53;&#x9A8C;&#xFF1A;&#x4EE5;&#x6C89;&#x6D78;&#x5F0F;&#x5B66;&#x4E60; &#x7B51;&#x7262;&#x5B89;&#x5168;&#x9632;&#x7EBF;-&#x65B0;&#x534E;&#x7F51;\n", "5. Na&#x161;e kon&#x161;pir&#xE1;cie s&#xFA; lep&#x161;ie ako va&#x161;e (p&#xED;&#x161;e Samo Marec)\n" ] } ], "source": [ "# Import data collection modules\n", "import sys\n", "sys.path.append(str(PROJECT_ROOT))\n", "\n", "from src.data_collector import collect_and_process_news\n", "\n", "# Define date range (6 months)\n", "start_date = \"2024-01-01\"\n", "end_date = \"2024-06-30\"\n", "\n", "print(f\"Collecting news data from {start_date} to {end_date}\")\n", "print(\"Using BigQuery with built-in cost controls...\")\n", "\n", "# Collect news data\n", "events_df = collect_and_process_news(\n", " start_date=start_date,\n", " end_date=end_date,\n", " force_refresh=False, # Force fresh data collection\n", " use_method='bigquery' # Use BigQuery method\n", ")\n", "\n", "print(f\"\\nData collection completed!\")\n", "print(f\"Shape: {events_df.shape}\")\n", "print(f\"Columns: {list(events_df.columns)}\")\n", "print(f\"Date range: {events_df['date'].min()} to {events_df['date'].max()}\")\n", "\n", "# Show sample data\n", "print(\"\\nSample headlines:\")\n", "sample_headlines = events_df['headline'].dropna().head(5)\n", "for i, headline in enumerate(sample_headlines, 1):\n", " print(f\"{i}. {headline}\")\n" ] }, { "cell_type": "markdown", "id": "440dea1b", "metadata": {}, "source": [ "## Step 2: Sentiment Processing\n", "\n", "Process the collected news data to extract sentiment features.\n" ] }, { "cell_type": "code", "execution_count": 3, "id": "3d9c7955", "metadata": {}, "outputs": [ { "name": "stdout", "output_type": "stream", "text": [ " Checking GPU availability...\n" ] }, { "name": "stderr", "output_type": "stream", "text": [ "2025-10-23 02:11:54,126 - INFO - Using device: cuda\n" ] }, { "name": "stdout", "output_type": "stream", "text": [ " GPU available: NVIDIA GeForce RTX 4060 Laptop GPU (8.0 GB)\n", "\n", " Initializing SentimentAnalyzer with device: cuda\n" ] }, { "name": "stderr", "output_type": "stream", "text": [ "2025-10-23 02:12:00,966 - INFO - Computing sentiment for 18133 uncached headlines\n" ] }, { "name": "stdout", "output_type": "stream", "text": [ "Processing sentiment analysis...\n", " Found cached sentiment data - using cached results for speed!\n", " This avoids the FinBERT processing time\n" ] }, { "name": "stderr", "output_type": "stream", "text": [ "Computing sentiment: 100%|| 142/142 [05:49<00:00, 2.46s/it]\n" ] }, { "name": "stdout", "output_type": "stream", "text": [ "Cache error: cannot reindex on an axis with duplicate labels\n", "Falling back to fresh computation...\n", "\n", "Sentiment data shape: (18200, 5)\n", "Sentiment columns: ['headline', 'p_negative', 'p_neutral', 'p_positive', 'polarity']\n", "\n", "Sentiment statistics:\n", "Mean polarity: 0.650\n", "Polarity range: -0.933 to 0.933\n" ] }, { "ename": "KeyError", "evalue": "'date'", "output_type": "error", "traceback": [ "\u001b[31m---------------------------------------------------------------------------\u001b[39m", "\u001b[31mKeyError\u001b[39m Traceback (most recent call last)", "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 69\u001b[39m\n\u001b[32m 66\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPolarity range: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentiment_df[\u001b[33m'\u001b[39m\u001b[33mpolarity\u001b[39m\u001b[33m'\u001b[39m].min()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msentiment_df[\u001b[33m'\u001b[39m\u001b[33mpolarity\u001b[39m\u001b[33m'\u001b[39m].max()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m 68\u001b[39m \u001b[38;5;66;03m# Create daily features using the correct method name\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m daily_features = \u001b[43msentiment_analyzer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_daily_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentiment_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m 71\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mDaily features created!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m 72\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDaily features shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdaily_features.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n", "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\Coding Projects\\Personal\\macro_sentiment_trading\\src\\sentiment_analyzer.py:159\u001b[39m, in \u001b[36mSentimentAnalyzer.compute_daily_features\u001b[39m\u001b[34m(self, sentiment_df)\u001b[39m\n\u001b[32m 151\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m 152\u001b[39m \u001b[33;03mCompute daily sentiment features strictly following the research math.\u001b[39;00m\n\u001b[32m 153\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m (...)\u001b[39m\u001b[32m 156\u001b[39m \u001b[33;03m DataFrame with daily features and all required lags, MAs, rolling stds, and sums.\u001b[39;00m\n\u001b[32m 157\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m 158\u001b[39m \u001b[38;5;66;03m# Aggregate by date\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m grouped = \u001b[43msentiment_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m 160\u001b[39m features = []\n\u001b[32m 161\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m date, group \u001b[38;5;129;01min\u001b[39;00m tqdm(grouped, desc=\u001b[33m\"\u001b[39m\u001b[33mComputing daily features\u001b[39m\u001b[33m\"\u001b[39m):\n", "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\Coding Projects\\Personal\\macro_sentiment_trading\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:9210\u001b[39m, in \u001b[36mDataFrame.groupby\u001b[39m\u001b[34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m 9207\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m 9208\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mYou have to supply one of \u001b[39m\u001b[33m'\u001b[39m\u001b[33mby\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlevel\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m9210\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m 9211\u001b[39m \u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m 9212\u001b[39m \u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m 9213\u001b[39m \u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m 9214\u001b[39m \u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m 9215\u001b[39m \u001b[43m \u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m 9216\u001b[39m \u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m 9217\u001b[39m \u001b[43m \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m 9218\u001b[39m \u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m 9219\u001b[39m \u001b[43m \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m 9220\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n", "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\Coding Projects\\Personal\\macro_sentiment_trading\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1331\u001b[39m, in \u001b[36mGroupBy.__init__\u001b[39m\u001b[34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[39m\n\u001b[32m 1328\u001b[39m \u001b[38;5;28mself\u001b[39m.dropna = dropna\n\u001b[32m 1330\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1331\u001b[39m grouper, exclusions, obj = \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m 1332\u001b[39m \u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m 1333\u001b[39m \u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m 1334\u001b[39m \u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m 1335\u001b[39m \u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m 1336\u001b[39m \u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m 1337\u001b[39m \u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m 1338\u001b[39m \u001b[43m \u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m 1339\u001b[39m \u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m 1341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib.no_default:\n\u001b[32m 1342\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping._passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper.groupings):\n", "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\Coding Projects\\Personal\\macro_sentiment_trading\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:1043\u001b[39m, in \u001b[36mget_grouper\u001b[39m\u001b[34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[39m\n\u001b[32m 1041\u001b[39m in_axis, level, gpr = \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m 1042\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[32m 1044\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr.key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m 1045\u001b[39m \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[32m 1046\u001b[39m exclusions.add(gpr.key)\n", "\u001b[31mKeyError\u001b[39m: 'date'" ] } ], "source": [ "# Import sentiment processing modules\n", "from src.sentiment_analyzer import SentimentAnalyzer\n", "import os\n", "import torch\n", "\n", "# Check GPU availability\n", "print(\" Checking GPU availability...\")\n", "if torch.cuda.is_available():\n", " gpu_name = torch.cuda.get_device_name(0)\n", " gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n", " print(f\" GPU available: {gpu_name} ({gpu_memory:.1f} GB)\")\n", " device = \"cuda\"\n", "else:\n", " print(\" No GPU available - using CPU (will be slow)\")\n", " device = \"cpu\"\n", "\n", "# Initialize sentiment analyzer with GPU if available\n", "print(f\"\\n Initializing SentimentAnalyzer with device: {device}\")\n", "sentiment_analyzer = SentimentAnalyzer(device=device)\n", "\n", "print(\"Processing sentiment analysis...\")\n", "\n", "# Check if we have cached sentiment data first\n", "cache_file = \"data/cache/sentiment_cache.pkl\"\n", "if os.path.exists(cache_file):\n", " print(\" Found cached sentiment data - using cached results for speed!\")\n", " print(\" This avoids the FinBERT processing time\")\n", " \n", " # Use cached sentiment data if available\n", " try:\n", " sentiment_df = sentiment_analyzer.compute_sentiment(events_df['headline'].tolist())\n", " print(f\"\\nSentiment analysis completed using cache!\")\n", " except Exception as e:\n", " print(f\"Cache error: {e}\")\n", " print(\"Falling back to fresh computation...\")\n", " sentiment_df = sentiment_analyzer.compute_sentiment(events_df['headline'].tolist())\n", "else:\n", " print(\" No cached sentiment data found\")\n", " \n", " if device == \"cpu\":\n", " print(\" WARNING: CPU processing will be very slow (2.5+ hours)!\")\n", " print(\" Options:\")\n", " print(\" 1. Use GPU if available\")\n", " print(\" 2. Use smaller sample for demo\")\n", " print(\" 3. Use pre-computed sentiment data\")\n", " \n", " # For demo purposes, let's use a smaller sample\n", " print(\"\\n Using sample of headlines for demo (100 headlines)...\")\n", " sample_headlines = events_df['headline'].dropna().head(100).tolist()\n", " sentiment_df = sentiment_analyzer.compute_sentiment(sample_headlines)\n", " else:\n", " print(\" Using GPU for fast processing...\")\n", " # Use larger batch size for GPU\n", " sentiment_df = sentiment_analyzer.compute_sentiment(\n", " events_df['headline'].tolist(), \n", " batch_size=256 # Larger batch size for GPU\n", " )\n", "\n", "print(f\"\\nSentiment data shape: {sentiment_df.shape}\")\n", "print(f\"Sentiment columns: {list(sentiment_df.columns)}\")\n", "\n", "# Show sentiment statistics\n", "if 'polarity' in sentiment_df.columns:\n", " print(f\"\\nSentiment statistics:\")\n", " print(f\"Mean polarity: {sentiment_df['polarity'].mean():.3f}\")\n", " print(f\"Polarity range: {sentiment_df['polarity'].min():.3f} to {sentiment_df['polarity'].max():.3f}\")\n", "\n", "# Create daily features using the correct method name\n", "daily_features = sentiment_analyzer.compute_daily_features(sentiment_df)\n", "\n", "print(f\"\\nDaily features created!\")\n", "print(f\"Daily features shape: {daily_features.shape}\")\n", "print(f\"Feature columns: {list(daily_features.columns)}\")\n" ] }, { "cell_type": "markdown", "id": "5fb6f6ef", "metadata": {}, "source": [ "## Performance Optimization Options\n", "\n", "** GPU Acceleration:**\n", "- **CUDA GPU**: 10-50x faster than CPU\n", "- **Batch Size**: 256 for GPU vs 128 for CPU\n", "- **Memory**: 4GB+ VRAM recommended\n", "\n", "** Caching Strategy:**\n", "- **Cache hits**: Instant results\n", "- **Cache misses**: Process and cache new headlines\n", "- **Cache file**: `data/cache/sentiment_cache.pkl`\n", "\n", "** Alternative Approaches:**\n", "1. **Use pre-computed sentiment data** (fastest)\n", "2. **Sample subset for demo** (100 headlines)\n", "3. **GPU processing** (10-50x faster)\n", "4. **CPU processing** (slow but works)\n" ] }, { "cell_type": "code", "execution_count": null, "id": "d4aa8258", "metadata": {}, "outputs": [], "source": [ "# GPU Status Check and Optimization\n", "print(\" GPU Status Check\")\n", "print(\"=\" * 50)\n", "\n", "# Check PyTorch CUDA availability\n", "print(f\"PyTorch version: {torch.__version__}\")\n", "print(f\"CUDA available: {torch.cuda.is_available()}\")\n", "\n", "if torch.cuda.is_available():\n", " print(f\"CUDA version: {torch.version.cuda}\")\n", " print(f\"GPU count: {torch.cuda.device_count()}\")\n", " for i in range(torch.cuda.device_count()):\n", " props = torch.cuda.get_device_properties(i)\n", " print(f\"GPU {i}: {props.name}\")\n", " print(f\" Memory: {props.total_memory / 1024**3:.1f} GB\")\n", " print(f\" Compute Capability: {props.major}.{props.minor}\")\n", " \n", " # Test GPU performance\n", " print(\"\\n Testing GPU performance...\")\n", " try:\n", " # Create a test tensor on GPU\n", " test_tensor = torch.randn(1000, 1000).cuda()\n", " start_time = torch.cuda.Event(enable_timing=True)\n", " end_time = torch.cuda.Event(enable_timing=True)\n", " \n", " start_time.record()\n", " result = torch.matmul(test_tensor, test_tensor)\n", " end_time.record()\n", " torch.cuda.synchronize()\n", " \n", " gpu_time = start_time.elapsed_time(end_time)\n", " print(f\"GPU matrix multiplication: {gpu_time:.2f} ms\")\n", " print(\" GPU is working properly!\")\n", " \n", " except Exception as e:\n", " print(f\" GPU test failed: {e}\")\n", " \n", "else:\n", " print(\" No CUDA GPU available\")\n", " print(\"\\n To enable GPU acceleration:\")\n", " print(\"1. Install CUDA toolkit: https://developer.nvidia.com/cuda-downloads\")\n", " print(\"2. Install PyTorch with CUDA: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n", " print(\"3. Restart Python kernel\")\n", "\n", "print(\"\\n Performance Expectations:\")\n", "print(\" CPU (your current setup): ~72 seconds per batch\")\n", "print(\" GPU (RTX 3080): ~2-5 seconds per batch\")\n", "print(\" GPU (RTX 4090): ~1-3 seconds per batch\")\n", "print(\" Total time for 18K headlines:\")\n", "print(\" - CPU: ~2.5 hours\")\n", "print(\" - GPU: ~5-15 minutes\")\n" ] }, { "cell_type": "markdown", "id": "3f00a38f", "metadata": {}, "source": [ "## Step 3: Market Data Processing\n", "\n", "Collect and align market data with sentiment features.\n" ] }, { "cell_type": "code", "execution_count": null, "id": "f4d112dd", "metadata": {}, "outputs": [], "source": [ "# Import market data processing modules\n", "from src.market_processor import MarketProcessor\n", "\n", "# Initialize market processor\n", "market_processor = MarketProcessor()\n", "\n", "# Define assets to collect\n", "assets = [\"EURUSD\", \"USDJPY\", \"TNOTE\"]\n", "\n", "print(f\"Collecting market data for: {', '.join(assets)}\")\n", "print(f\"Date range: {start_date} to {end_date}\")\n", "\n", "# Collect market data for all assets\n", "print(\"\\nCollecting market data...\")\n", "market_data = market_processor.fetch_market_data(\n", " start_date=start_date,\n", " end_date=end_date\n", ")\n", "\n", "print(f\"Market data collected for: {list(market_data.keys())}\")\n", "\n", "# Add market features to each asset\n", "for asset_name, asset_data in market_data.items():\n", " print(f\"\\nComputing features for {asset_name}...\")\n", " market_data[asset_name] = market_processor.compute_market_features(asset_data)\n", " print(f\"{asset_name} data shape: {asset_data.shape}\")\n", "\n", "# Align market data with sentiment features\n", "print(\"\\nAligning market data with sentiment features...\")\n", "aligned_data = market_processor.align_features(market_data, daily_features)\n", "\n", "print(f\"\\nData alignment completed!\")\n", "print(f\"Aligned datasets: {list(aligned_data.keys())}\")\n", "for asset_name, asset_data in aligned_data.items():\n", " print(f\" {asset_name}: {asset_data.shape}\")\n" ] }, { "cell_type": "markdown", "id": "ddce0219", "metadata": {}, "source": [ "## Step 4: Model Training\n", "\n", "Train machine learning models on the aligned data.\n" ] }, { "cell_type": "code", "execution_count": null, "id": "b84d5355", "metadata": {}, "outputs": [], "source": [ "# Import model training modules\n", "from src.model_trainer import ModelTrainer\n", "from src.model_persistence import ModelPersistence\n", "\n", "# Initialize model trainer and persistence\n", "model_trainer = ModelTrainer()\n", "model_persistence = ModelPersistence()\n", "\n", "print(\"Training models on aligned data...\")\n", "\n", "# Train models for each asset\n", "trained_models = {}\n", "\n", "for asset_name, asset_data in aligned_data.items():\n", " print(f\"\\nTraining models for {asset_name}...\")\n", " print(f\"Data shape: {asset_data.shape}\")\n", " \n", " # Train models\n", " models, scalers, feature_columns = model_trainer.train_models(asset_data)\n", " \n", " # Store trained models\n", " trained_models[asset_name] = {\n", " 'models': models,\n", " 'scalers': scalers,\n", " 'feature_columns': feature_columns\n", " }\n", " \n", " print(f\"Trained {len(models)} models for {asset_name}\")\n", " print(f\"Model types: {list(models.keys())}\")\n", " print(f\"Feature columns: {len(feature_columns)}\")\n", "\n", "print(f\"\\nModel training completed!\")\n", "print(f\"Trained models for: {list(trained_models.keys())}\")\n" ] }, { "cell_type": "markdown", "id": "fc83b4f6", "metadata": {}, "source": [ "## Step 5: Backtesting\n", "\n", "Test model performance using backtesting.\n" ] }, { "cell_type": "code", "execution_count": null, "id": "ef55084a", "metadata": {}, "outputs": [], "source": [ "# Import backtesting modules\n", "from src.multi_timeframe_backtester import MultiTimeframeBacktester\n", "\n", "# Initialize performance analyzer\n", "performance_analyzer = PerformanceAnalyzer()\n", "\n", "print(\"Running backtesting analysis...\")\n", "\n", "# Run backtesting for each asset\n", "backtest_results = {}\n", "\n", "for asset_name, asset_data in aligned_data.items():\n", " print(f\"\\nRunning backtest for {asset_name}...\")\n", " \n", " # Get trained models for this asset\n", " asset_models = trained_models[asset_name]['models']\n", " asset_scalers = trained_models[asset_name]['scalers']\n", " asset_features = trained_models[asset_name]['feature_columns']\n", " \n", " # Run backtest\n", " try:\n", " backtest_result = performance_analyzer.run_backtest(\n", " data=asset_data,\n", " models=asset_models,\n", " scalers=asset_scalers,\n", " feature_columns=asset_features\n", " )\n", " backtest_results[asset_name] = backtest_result\n", " print(f\"Backtest completed for {asset_name}\")\n", " \n", " # Show key metrics\n", " if 'accuracy' in backtest_result:\n", " print(f\"Accuracy: {backtest_result['accuracy']:.3f}\")\n", " if 'sharpe_ratio' in backtest_result:\n", " print(f\"Sharpe Ratio: {backtest_result['sharpe_ratio']:.3f}\")\n", " \n", " except Exception as e:\n", " print(f\"Backtest failed for {asset_name}: {e}\")\n", "\n", "print(f\"\\nBacktesting completed!\")\n", "print(f\"Backtest results for: {list(backtest_results.keys())}\")\n" ] }, { "cell_type": "markdown", "id": "f3ad0a90", "metadata": {}, "source": [ "## Step 6: Signal Generation\n", "\n", "Generate current trading signals using trained models.\n" ] }, { "cell_type": "code", "execution_count": null, "id": "e06b96d4", "metadata": {}, "outputs": [], "source": [ "# Import signal generation modules\n", "from src.comprehensive_signal_generator import ComprehensiveSignalGenerator\n", "\n", "# Initialize signal generator\n", "signal_generator = ComprehensiveSignalGenerator()\n", "\n", "print(\"Generating current trading signals...\")\n", "\n", "# Generate signals for all assets\n", "try:\n", " signals = signal_generator.generate_all_signals()\n", " print(f\"Signals generated successfully!\")\n", " print(f\"Assets with signals: {list(signals.keys())}\")\n", " \n", " # Show signal summary for each asset\n", " for asset_name, asset_signals in signals.items():\n", " print(f\"\\n{asset_name} signals:\")\n", " if isinstance(asset_signals, dict):\n", " for model_type, signal_data in asset_signals.items():\n", " if isinstance(signal_data, dict) and 'signal' in signal_data:\n", " print(f\" {model_type}: {signal_data['signal']} (confidence: {signal_data.get('confidence', 'N/A')})\")\n", " \n", "except Exception as e:\n", " print(f\"Signal generation failed: {e}\")\n", " signals = {}\n", "\n", "print(f\"\\nSignal generation completed!\")\n", "print(f\"Signals generated for: {list(signals.keys())}\")\n" ] }, { "cell_type": "markdown", "id": "cced2d4e", "metadata": {}, "source": [ "## Results Summary\n", "\n", "Display a comprehensive summary of the entire pipeline results.\n" ] }, { "cell_type": "code", "execution_count": null, "id": "c38dbdc8", "metadata": {}, "outputs": [], "source": [ "print(\"=\" * 80)\n", "print(\" PIPELINE RESULTS SUMMARY\")\n", "print(\"=\" * 80)\n", "print(f\"Generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n", "print(\"=\" * 80)\n", "\n", "# Data Collection Summary\n", "print(\"\\n DATA COLLECTION:\")\n", "print(f\" Date Range: {start_date} to {end_date}\")\n", "print(f\" Events Data: {events_df.shape[0]:,} records\")\n", "print(f\" Headlines: {events_df['headline'].notna().sum():,} valid headlines\")\n", "print(f\" Cost: $0.00 (within free tier)\")\n", "\n", "# Sentiment Processing Summary\n", "print(\"\\n SENTIMENT PROCESSING:\")\n", "print(f\" Sentiment Records: {sentiment_df.shape[0]:,}\")\n", "print(f\" Daily Features: {daily_features.shape[0]:,} days\")\n", "print(f\" Feature Columns: {daily_features.shape[1]}\")\n", "\n", "# Market Data Summary\n", "print(\"\\n MARKET DATA:\")\n", "for asset, data in aligned_data.items():\n", " print(f\" {asset}: {data.shape[0]:,} records\")\n", "\n", "# Model Training Summary\n", "print(\"\\n MODEL TRAINING:\")\n", "for asset, models_info in trained_models.items():\n", " models = models_info['models']\n", " print(f\" {asset}: {len(models)} models ({', '.join(models.keys())})\")\n", "\n", "# Backtesting Summary\n", "print(\"\\n BACKTESTING:\")\n", "for asset, result in backtest_results.items():\n", " print(f\" {asset}: Backtest completed\")\n", " if 'accuracy' in result:\n", " print(f\" - Accuracy: {result['accuracy']:.3f}\")\n", " if 'sharpe_ratio' in result:\n", " print(f\" - Sharpe Ratio: {result['sharpe_ratio']:.3f}\")\n", "\n", "# Signal Generation Summary\n", "print(\"\\n SIGNAL GENERATION:\")\n", "for asset, asset_signals in signals.items():\n", " print(f\" {asset}: Signals generated\")\n", " if asset_signals:\n", " print(f\" - Signal types: {', '.join(asset_signals.keys())}\")\n", "\n", "print(\"\\n\" + \"=\" * 80)\n", "print(\" PIPELINE COMPLETED SUCCESSFULLY\")\n", "print(\" ALL MODELS TRAINED AND TESTED\")\n", "print(\" SIGNALS GENERATED\")\n", "print(\" COST: $0.00 (within free tier)\")\n", "print(\"=\" * 80)\n" ] }, { "cell_type": "markdown", "id": "4ea34d04", "metadata": {}, "source": [ "## Next Steps\n", "\n", "1. **Set up budget alerts** at https://console.cloud.google.com/billing\n", "2. **Use CLI commands** for production data collection\n", "3. **Monitor model performance** regularly\n", "4. **Generate signals** on a schedule\n", "\n", "### CLI Commands for Production:\n", "```bash\n", "# Collect 10 years of data (safe - will cost $0.00)\n", "python cli/main.py collect-news --start-date 2014-01-01 --end-date 2024-12-31 --method bigquery\n", "\n", "# Process sentiment\n", "python cli/main.py process-sentiment --data-path data/news/events_data_20140101_20241231.parquet\n", "\n", "# Process market data\n", "python cli/main.py process-market --start-date 2014-01-01 --end-date 2024-12-31\n", "\n", "# Train models\n", "python cli/main.py train-models --data-path results/20140101_20241231/\n", "\n", "# Generate signals\n", "python cli/main.py get-signals --assets EURUSD USDJPY TNOTE\n", "```\n" ] } ], "metadata": { "kernelspec": { "display_name": ".venv", "language": "python", "name": "python3" }, "language_info": { "codemirror_mode": { "name": "ipython", "version": 3 }, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.6" } }, "nbformat": 4, "nbformat_minor": 5 } 