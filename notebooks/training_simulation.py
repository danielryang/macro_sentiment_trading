#!/usr/bin/env python3 """ Macro Sentiment Trading - Complete Lifecycle Demo (Python Script Version) This script replicates the exact functionality of the 01_training_simulation.ipynb notebook using direct Python method calls instead of CLI commands. Complete Lifecycle: 1. Data Collection - Real GDELT news data via BigQuery 2. Market Data - Real financial data from Yahoo Finance 3. Data Alignment - Combine news sentiment with market data 4. Model Training - Train ML models with proper validation 5. Backtesting - Validate models using historical data 6. Signal Generation - Generate real-time trading signals 7. Visualization - Generate charts and performance plots 8. Model Management - Complete model lifecycle management """ import sys import os from pathlib import Path import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from datetime import datetime, timedelta import warnings warnings.filterwarnings('ignore') # Add project root to Python path current_dir = Path.cwd() if current_dir.name == 'notebooks': project_root = current_dir.parent else: project_root = current_dir sys.path.insert(0, str(project_root)) print(f"Project root: {project_root}") # Import project modules (working imports only) from src.data_collector import UnifiedGDELTCollector, collect_and_process_news from src.market_processor import MarketProcessor from src.sentiment_analyzer import SentimentAnalyzer from src.model_trainer import ModelTrainer from src.model_persistence import ModelPersistence, save_model, load_model, list_models from src.performance_metrics import PerformanceAnalyzer from src.alpha_analytics import AlphaAnalytics # Configure matplotlib for inline plotting plt.style.use('default') plt.rcParams['figure.figsize'] = (12, 8) plt.rcParams['font.size'] = 10 print(" Setup complete - ready to run the complete lifecycle!") # Training Configuration # ===================== # CONFIGURE YOUR SETTINGS HERE # ===================== CONFIG = { # Date Range Configuration 'start_date': '2023-07-01', # Start of training period 'end_date': '2023-12-31', # End of training period # Assets to train models for 'assets': ['EURUSD', 'USDJPY', 'TNOTE'], # Models to train 'models': ['logistic', 'xgboost'], # Output directory for results 'output_dir': 'data/notebook_demo' } print(" Training Configuration:") print(f" Date Range: {CONFIG['start_date']} to {CONFIG['end_date']}") print(f" Assets: {', '.join(CONFIG['assets'])}") print(f" Models: {', '.join(CONFIG['models'])}") print(f" Output: {CONFIG['output_dir']}") # Calculate training period start_dt = datetime.strptime(CONFIG['start_date'], '%Y-%m-%d') end_dt = datetime.strptime(CONFIG['end_date'], '%Y-%m-%d') training_days = (end_dt - start_dt).days print(f" Training Period: {training_days} days ({training_days/365.25:.1f} years)") print("\n To change settings, modify the CONFIG dictionary above:") print(" - Change 'start_date' and 'end_date' for different time periods") print(" - Add/remove assets in the 'assets' list") print(" - Add/remove models in the 'models' list") print(" - Change 'output_dir' for different output location") def step_2a_collect_news(): """Step 2a: Collect News Data""" print("\n Step 2a: Collecting News Data from GDELT...") print("=" * 50) # Initialize data collector data_collector = UnifiedGDELTCollector() # Collect news data using direct method call print(f" Date range: {CONFIG['start_date']} to {CONFIG['end_date']}") print(" Collecting news data (this may take 1-2 minutes)...") try: # Use the collect_and_process_news function directly events_df = collect_and_process_news( start_date=CONFIG['start_date'], end_date=CONFIG['end_date'], force_refresh=True ) print(f" Collected {len(events_df):,} news events") print(f" Data shape: {events_df.shape}") if 'date' in events_df.columns: print(f" Date range: {events_df['date'].min()} to {events_df['date'].max()}") # Show sample data print("\n Sample headlines:") for i, headline in enumerate(events_df['headline'].dropna().head(3)): print(f" {i+1}. {headline[:100]}...") return events_df except Exception as e: print(f" Error collecting news data: {e}") return None def step_2b_collect_market_data(): """Step 2b: Collect Market Data""" print("\n Step 2b: Collecting Market Data from Yahoo Finance...") print("=" * 50) # Initialize market processor market_processor = MarketProcessor() # Collect market data using direct method call print(f" Assets: {', '.join(CONFIG['assets'])}") print(" Collecting market data...") try: # Use the fetch_market_data method directly market_data = market_processor.fetch_market_data( start_date=CONFIG['start_date'], end_date=CONFIG['end_date'] ) print(f" Collected market data for {len(market_data)} assets") # Display market data summary print("\n Market Data Summary:") for asset_name, data in market_data.items(): if len(data) > 0: print(f" {asset_name}: {len(data):,} records, {data.index.min()} to {data.index.max()}") print(f" Columns: {list(data.columns)[:5]}...") # Show first 5 columns else: print(f" {asset_name}: No data available") return market_data except Exception as e: print(f" Error collecting market data: {e}") return None def step_2c_process_sentiment(events_df): """Step 2c: Process Sentiment Data""" print("\n Step 2c: Processing Sentiment Data...") print("=" * 50) # Initialize sentiment analyzer sentiment_analyzer = SentimentAnalyzer() # Process sentiment data using direct method call print(" Processing sentiment data...") try: # Use the compute_sentiment method directly sentiment_df = sentiment_analyzer.compute_sentiment(events_df['headline'].tolist()) # Add date column if events_df has date column if 'date' in events_df.columns: sentiment_df['date'] = events_df['date'].values print(f" Processed sentiment for {len(sentiment_df):,} headlines") print(f" Sentiment columns: {list(sentiment_df.columns)}") # Compute daily sentiment features print(" Computing daily sentiment features...") daily_features = sentiment_analyzer.compute_daily_features(sentiment_df) print(f" Computed daily features for {len(daily_features):,} days") # Display sentiment summary print("\n Sentiment Analysis Summary:") if 'date' in sentiment_df.columns: print(f" Date range: {sentiment_df['date'].min()} to {sentiment_df['date'].max()}") print(f" Total headlines: {len(sentiment_df):,}") print(f" Daily features: {len(daily_features):,} days") if 'polarity' in sentiment_df.columns: print(f" Positive sentiment: {(sentiment_df['polarity'] > 0).sum():,}") print(f" Negative sentiment: {(sentiment_df['polarity'] < 0).sum():,}") print(f" Neutral sentiment: {(sentiment_df['polarity'] == 0).sum():,}") else: print(f" Available columns: {list(sentiment_df.columns)}") return sentiment_df, daily_features except Exception as e: print(f" Error processing sentiment data: {e}") return None, None def step_2d_align_data(market_data, daily_features): """Step 2d: Align Data""" print("\n Step 2d: Aligning Sentiment and Market Data...") print("=" * 50) # Align features using direct method call print(" Aligning features...") try: # Initialize market processor for alignment market_processor = MarketProcessor() # Use the align_features method directly aligned_data = market_processor.align_features(market_data, daily_features) print(f" Aligned features for {len(aligned_data)} assets") # Display aligned data summary print("\n Aligned Data Summary:") for asset_name, data in aligned_data.items(): print(f" {asset_name}: {len(data):,} records, {data.columns.tolist()[:5]}...") return aligned_data except Exception as e: print(f" Error aligning data: {e}") return None def step_3a_train_models(aligned_data): """Step 3a: Train Models""" print("\n Step 3a: Training Machine Learning Models...") print("=" * 50) # Initialize model trainer model_trainer = ModelTrainer() # Train models using direct method call print(f" Models: {', '.join(CONFIG['models'])}") print(" Training models (this may take 2-3 minutes)...") try: # Use the train_models method directly trained_models = model_trainer.train_models( aligned_data=aligned_data, models=CONFIG['models'], output_dir=CONFIG['output_dir'] ) print(f" Trained {len(trained_models)} models") # Display model summary print("\n Model Training Summary:") for asset_name, models in trained_models.items(): print(f" {asset_name}: {list(models.keys())}") return trained_models except Exception as e: print(f" Error training models: {e}") return None def step_4a_run_backtesting(aligned_data, trained_models): """Step 4a: Run Backtesting""" print("\n Step 4a: Running Backtesting Analysis...") print("=" * 50) # Initialize performance analyzer performance_analyzer = PerformanceAnalyzer() # Run backtesting using direct method call print(" Running backtest...") try: # Use the run_backtest method directly backtest_results = performance_analyzer.run_backtest( aligned_data=aligned_data, trained_models=trained_models, start_date=CONFIG['start_date'], end_date=CONFIG['end_date'] ) print(f" Backtest completed for {len(backtest_results)} assets") # Display backtest summary print("\n Backtest Results Summary:") for asset_name, results in backtest_results.items(): print(f" {asset_name}:") print(f" Total Return: {results['total_return']:.2%}") print(f" Sharpe Ratio: {results['sharpe_ratio']:.2f}") print(f" Max Drawdown: {results['max_drawdown']:.2%}") return backtest_results except Exception as e: print(f" Error running backtest: {e}") return None def step_5a_generate_signals(aligned_data, trained_models): """Step 5a: Generate Trading Signals""" print("\n Step 5a: Generating Trading Signals...") print("=" * 50) # Generate signals using model predictions print(" Generating signals...") try: # Use trained models to generate signals signals = {} for asset_name in CONFIG['assets']: if asset_name in trained_models and aligned_data and asset_name in aligned_data: asset_data = aligned_data[asset_name] asset_models = trained_models[asset_name] # Generate signals for each model asset_signals = {} for model_name, model_info in asset_models.items(): model = model_info['model'] scaler = model_info['scaler'] feature_columns = model_info['feature_columns'] # Prepare features for prediction X = asset_data[feature_columns].fillna(0) X_scaled = scaler.transform(X) # Generate predictions predictions = model.predict(X_scaled) probabilities = model.predict_proba(X_scaled) if hasattr(model, 'predict_proba') else None # Convert to signals (1: buy, -1: sell, 0: hold) signal_values = [] for i, pred in enumerate(predictions): if pred == 1: # Positive prediction signal_values.append(1) # Buy elif pred == 0: # Negative prediction signal_values.append(-1) # Sell else: signal_values.append(0) # Hold asset_signals[model_name] = { 'signals': signal_values, 'predictions': predictions, 'probabilities': probabilities } signals[asset_name] = asset_signals print(f" Generated signals for {len(signals)} assets") # Display signals summary print("\n Signals Summary:") for asset_name, asset_signals in signals.items(): print(f" {asset_name}:") for model_name, model_signals in asset_signals.items(): buy_signals = sum(1 for s in model_signals['signals'] if s == 1) sell_signals = sum(1 for s in model_signals['signals'] if s == -1) hold_signals = sum(1 for s in model_signals['signals'] if s == 0) print(f" {model_name}: {buy_signals} buy, {sell_signals} sell, {hold_signals} hold") return signals except Exception as e: print(f" Error generating signals: {e}") return None def step_6a_generate_visualizations(aligned_data, backtest_results, signals): """Step 6a: Generate Visualizations""" print("\n Step 6a: Generating Visualizations...") print("=" * 50) # Generate visualizations using direct method call print(" Generating visualizations...") try: # Use the AlphaAnalytics module for visualizations alpha_analytics = AlphaAnalytics() # Generate visualizations for each asset viz_results = {} for asset in CONFIG['assets']: if asset in aligned_data: viz_path = alpha_analytics.generate_analytics( data=aligned_data[asset], asset_name=asset, output_dir=CONFIG['output_dir'] ) viz_results[asset] = viz_path print(f" Generated {len(viz_results)} visualizations") # Display visualization summary print("\n Visualization Summary:") for asset, viz_path in viz_results.items(): print(f" {asset}: {viz_path}") return viz_results except Exception as e: print(f" Error generating visualizations: {e}") return None def step_7a_model_management(): """Step 7a: Model Management Operations""" print("\n Step 7a: Model Management Operations...") print("=" * 50) # Initialize model persistence model_persistence = ModelPersistence() # List available models print(" Listing available models...") try: available_models = model_persistence.list_models() print(f" Found {len(available_models)} models") # Display model summary print("\n Available Models:") for model_info in available_models: print(f" {model_info['model_id']}: {model_info['asset']} - {model_info['model_type']}") except Exception as e: print(f" Error listing models: {e}") available_models = None # Show model details print("\n Showing model details...") try: if available_models and len(available_models) > 0: latest_model = available_models[0]['model_id'] model_metadata = model_persistence.get_model_metadata(latest_model) print(f" Model details for {latest_model}") print(f" Asset: {model_metadata.get('asset', 'N/A')}") print(f" Type: {model_metadata.get('model_type', 'N/A')}") print(f" Created: {model_metadata.get('created_at', 'N/A')}") except Exception as e: print(f" Error showing model details: {e}") def step_8a_pipeline_summary(): """Step 8a: Complete Pipeline Summary""" print("\n Complete Lifecycle Demo Summary") print("=" * 50) print("\n What We Accomplished:") print("\n1. Data Collection:") print(" - Collected real GDELT news data via BigQuery") print(" - Downloaded real market data from Yahoo Finance") print(" - Processed sentiment analysis using FinBERT") print("\n2. Data Alignment:") print(" - Combined news sentiment with market data") print(" - Created predictive features and targets") print(" - Aligned data by date for all assets") print("\n3. Model Training:") print(" - Trained 2 models for 3 assets") print(" - Used proper temporal validation (expanding window)") print(" - Generated SHAP analysis for model interpretability") print("\n4. Backtesting:") print(" - Validated models using historical data") print(" - Generated performance metrics and visualizations") print(" - Created comprehensive backtest reports") print("\n5. Signal Generation:") print(" - Generated real-time trading signals") print(" - Applied risk management and position sizing") print(" - Created signal quality metrics") print("\n6. Visualization:") print(" - Generated performance charts and plots") print(" - Created model interpretability dashboards") print(" - Produced comprehensive analytics reports") print("\n7. Model Management:") print(" - Listed and managed trained models") print(" - Tracked model performance and metadata") print(" - Enabled model comparison and selection") print("\n" + "=" * 50) print(" Complete Lifecycle Demo Finished Successfully!") print("=" * 50) def main(): """Run the complete pipeline""" print(" Starting Complete Macro Sentiment Trading Pipeline...") print("=" * 60) # Step 1: Data Collection events_df = step_2a_collect_news() if events_df is None: print(" Failed to collect news data. Exiting.") return False market_data = step_2b_collect_market_data() if market_data is None: print(" Failed to collect market data. Exiting.") return False # Step 2: Sentiment Processing sentiment_df, daily_features = step_2c_process_sentiment(events_df) if sentiment_df is None or daily_features is None: print(" Failed to process sentiment data. Exiting.") return False # Step 3: Data Alignment aligned_data = step_2d_align_data(market_data, daily_features) if aligned_data is None: print(" Failed to align data. Exiting.") return False # Step 4: Model Training trained_models = step_3a_train_models(aligned_data) if trained_models is None: print(" Failed to train models. Exiting.") return False # Step 5: Backtesting backtest_results = step_4a_run_backtesting(aligned_data, trained_models) if backtest_results is None: print(" Failed to run backtesting. Exiting.") return False # Step 6: Signal Generation signals = step_5a_generate_signals(aligned_data, trained_models) if signals is None: print(" Failed to generate signals. Exiting.") return False # Step 7: Visualization viz_results = step_6a_generate_visualizations(aligned_data, backtest_results, signals) if viz_results is None: print(" Failed to generate visualizations. Exiting.") return False # Step 8: Model Management step_7a_model_management() # Step 9: Pipeline Summary step_8a_pipeline_summary() print("\n Complete pipeline executed successfully!") return True if __name__ == "__main__": success = main() sys.exit(0 if success else 1) 