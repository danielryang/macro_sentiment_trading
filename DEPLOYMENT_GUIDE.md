# Deployment Guide for Macro Sentiment Trading Pipeline ## Overview This guide provides comprehensive instructions for deploying the Macro Sentiment Trading Pipeline in production environments. The system has been enhanced with AI-powered testing, monitoring, and deployment preparation tools. ## Pre-Deployment Checklist ### Critical Security Checks - [ ] All credential files removed from repository - [ ] `.env` file added to `.gitignore` - [ ] Environment variables properly configured - [ ] API keys stored securely (not in code) - [ ] Service account keys properly managed ### Configuration Validation - [ ] Configuration validator passes all checks - [ ] Date ranges are valid (no future dates) - [ ] All required directories exist and are writable - [ ] Environment variables validated ### Model Persistence - [ ] Model registry functional - [ ] Model saving/loading tested - [ ] Model versioning implemented - [ ] Model artifacts properly organized ### Testing - [ ] All automated tests pass - [ ] Integration tests successful - [ ] Performance tests within acceptable limits - [ ] System tests validate deployment readiness ### Dependencies - [ ] All dependencies reconciled between requirements.txt and setup.py - [ ] Pinned versions available for production (requirements-pinned.txt) - [ ] Optional dependencies properly separated - [ ] Development dependencies isolated ### Health Monitoring - [ ] Health check endpoints functional - [ ] System metrics collection working - [ ] Monitoring server can start - [ ] All component health checks pass ## Deployment Methods ### Method 1: Docker Deployment (Recommended) #### 1. Build Docker Image ```bash # Build the image docker build -t macro-sentiment-trading:latest . # Verify the build docker images | grep macro-sentiment-trading ``` #### 2. Prepare Environment ```bash # Create environment file cp env.example.new .env # Edit .env with your configuration nano .env ``` #### 3. Run Container ```bash # Production deployment docker run -d \ --name macro-sentiment-trading \ -v $(pwd)/data:/app/data \ -v $(pwd)/results:/app/results \ -v $(pwd)/logs:/app/logs \ --env-file .env \ -p 8080:8080 \ macro-sentiment-trading:latest # Check container status docker ps docker logs macro-sentiment-trading ``` #### 4. Health Check ```bash # Check health endpoints curl http://localhost:8080/health curl http://localhost:8080/health/all curl http://localhost:8080/metrics ``` ### Method 2: Virtual Environment Deployment #### 1. Set Up Environment ```bash # Create virtual environment python -m venv venv source venv/bin/activate # On Windows: venv\Scripts\activate # Install dependencies (pinned versions for production) pip install -r requirements-pinned.txt # Install the package pip install -e . ``` #### 2. Configure Environment ```bash # Copy and configure environment cp env.example.new .env # Edit .env with your settings ``` #### 3. Validate Setup ```bash # Run configuration validation python src/config_validator.py # Run health checks python src/health_monitor.py --check-all # Run deployment readiness tests python scripts/run_tests.py --deployment ``` #### 4. Start Services ```bash # Start the health monitor python src/health_monitor.py --server --port 8080 & # Run the pipeline (example) python cli/main.py run-pipeline \ --start-date 2024-01-01 \ --end-date 2024-12-31 \ --assets EURUSD USDJPY \ --models xgboost logistic ``` ### Method 3: Cloud Deployment #### Google Cloud Platform (GCP) 1. **Prepare Cloud Resources** ```bash # Set up BigQuery (if using) gcloud auth login gcloud config set project YOUR_PROJECT_ID # Create service account gcloud iam service-accounts create macro-sentiment-trading \ --display-name="Macro Sentiment Trading Service Account" # Grant permissions gcloud projects add-iam-policy-binding YOUR_PROJECT_ID \ --member="serviceAccount:macro-sentiment-trading@YOUR_PROJECT_ID.iam.gserviceaccount.com" \ --role="roles/bigquery.user" ``` 2. **Deploy to Cloud Run** ```bash # Build and push image docker build -t gcr.io/YOUR_PROJECT_ID/macro-sentiment-trading . docker push gcr.io/YOUR_PROJECT_ID/macro-sentiment-trading # Deploy to Cloud Run gcloud run deploy macro-sentiment-trading \ --image gcr.io/YOUR_PROJECT_ID/macro-sentiment-trading \ --platform managed \ --region us-central1 \ --set-env-vars GDELT_METHOD=bigquery \ --set-env-vars GOOGLE_CLOUD_PROJECT=YOUR_PROJECT_ID ``` #### AWS Deployment 1. **Prepare AWS Resources** ```bash # Configure AWS CLI aws configure # Create ECR repository aws ecr create-repository --repository-name macro-sentiment-trading ``` 2. **Deploy to ECS/Fargate** ```bash # Build and push to ECR aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin YOUR_ACCOUNT.dkr.ecr.us-east-1.amazonaws.com docker build -t macro-sentiment-trading . docker tag macro-sentiment-trading:latest YOUR_ACCOUNT.dkr.ecr.us-east-1.amazonaws.com/macro-sentiment-trading:latest docker push YOUR_ACCOUNT.dkr.ecr.us-east-1.amazonaws.com/macro-sentiment-trading:latest # Create ECS task definition and service (use AWS Console or CLI) ``` ## Production Configuration ### Environment Variables Create a `.env` file with the following variables: ```bash # Data Collection GDELT_METHOD=bigquery # or 'free' GOOGLE_CLOUD_PROJECT=your-project-id GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json BIGQUERY_MAX_COST_USD=10.00 # Performance MAX_WORKERS=4 CACHE_SENTIMENT=true BATCH_SIZE=128 # Logging LOG_LEVEL=INFO # Model Configuration DEFAULT_TRAINING_WINDOW=730 DEFAULT_TEST_WINDOW=365 # Optional API Keys (for enhanced sentiment analysis) # OPENAI_API_KEY=sk-your-key-here # ANTHROPIC_API_KEY=sk-ant-your-key-here ``` ### Resource Requirements #### Minimum Requirements - **CPU**: 2 cores - **RAM**: 4GB - **Disk**: 10GB free space - **Network**: Stable internet connection #### Recommended for Production - **CPU**: 4+ cores - **RAM**: 8GB+ - **Disk**: 50GB+ SSD - **GPU**: Optional (for faster FinBERT inference) ### Security Configuration #### 1. Credential Management ```bash # Store credentials securely export GOOGLE_APPLICATION_CREDENTIALS="/secure/path/to/service-account.json" export OPENAI_API_KEY="$(cat /secure/path/to/openai-key)" # Use secret management services in production # AWS Secrets Manager, Google Secret Manager, Azure Key Vault ``` #### 2. Network Security ```bash # Firewall rules (example for iptables) iptables -A INPUT -p tcp --dport 8080 -s TRUSTED_IP_RANGE -j ACCEPT iptables -A INPUT -p tcp --dport 8080 -j DROP # Use reverse proxy (nginx example) server { listen 80; server_name your-domain.com; location / { proxy_pass http://localhost:8080; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; } } ``` ## Monitoring and Maintenance ### Health Monitoring #### 1. Built-in Health Checks ```bash # Check system health curl http://localhost:8080/health # Comprehensive health check curl http://localhost:8080/health/all # System metrics curl http://localhost:8080/metrics # Component-specific checks curl http://localhost:8080/health/system curl http://localhost:8080/health/models curl http://localhost:8080/health/configuration ``` #### 2. External Monitoring Integration ```bash # Prometheus metrics (if integrated) curl http://localhost:8080/metrics/prometheus # Custom monitoring script python src/health_monitor.py --check-all --json > health_status.json ``` ### Log Management #### 1. Log Locations - **Application Logs**: `logs/pipeline.log` - **Health Monitor Logs**: `logs/health_monitor.log` - **Error Logs**: `logs/error.log` #### 2. Log Rotation ```bash # Setup logrotate (Linux) cat > /etc/logrotate.d/macro-sentiment-trading << EOF /path/to/logs/*.log { daily missingok rotate 30 compress delaycompress notifempty copytruncate } EOF ``` ### Performance Monitoring #### 1. System Metrics ```bash # Monitor resource usage python src/health_monitor.py --check-system # Performance testing python scripts/run_tests.py --performance ``` #### 2. Model Performance ```bash # List available models python src/model_persistence.py list # Check model performance python src/model_persistence.py show MODEL_ID ``` ## Troubleshooting ### Common Issues #### 1. Import Errors ```bash # Check dependencies python src/health_monitor.py --check-deps # Reinstall dependencies pip install -r requirements-pinned.txt --force-reinstall ``` #### 2. Configuration Issues ```bash # Validate configuration python src/config_validator.py # Check environment variables env | grep -E "(GDELT|GOOGLE|OPENAI|ANTHROPIC)" ``` #### 3. Permission Errors ```bash # Check directory permissions python src/health_monitor.py --check-all # Fix permissions chmod -R 755 data/ results/ logs/ ``` #### 4. Memory Issues ```bash # Monitor memory usage python src/health_monitor.py --check-system # Reduce batch size export BATCH_SIZE=64 # Enable model caching export CACHE_SENTIMENT=true ``` ### Recovery Procedures #### 1. Service Recovery ```bash # Restart container docker restart macro-sentiment-trading # Check logs docker logs macro-sentiment-trading --tail 100 # Health check curl http://localhost:8080/health ``` #### 2. Data Recovery ```bash # Check data integrity python scripts/run_tests.py --deployment # Restore from backup cp backup/data/* data/ cp backup/results/* results/ ``` #### 3. Model Recovery ```bash # List available models python src/model_persistence.py list # Load latest model python -c " from src.model_persistence import load_latest_model model, scaler, features, metadata = load_latest_model('EURUSD', 'xgboost') print(f'Loaded model: {metadata}') " ``` ## Scaling and Optimization ### Horizontal Scaling #### 1. Load Balancing ```bash # Use multiple instances behind a load balancer # Example with nginx upstream macro_sentiment { server instance1:8080; server instance2:8080; server instance3:8080; } ``` #### 2. Database Scaling ```bash # Use distributed storage for model registry # Example: MongoDB, PostgreSQL with replication ``` ### Performance Optimization #### 1. Caching ```bash # Enable all caching export CACHE_SENTIMENT=true export CACHE_MARKET_DATA=true # Use Redis for distributed caching (if implemented) export REDIS_URL=redis://localhost:6379 ``` #### 2. Parallel Processing ```bash # Increase worker threads export MAX_WORKERS=8 # Use GPU acceleration export CUDA_VISIBLE_DEVICES=0 ``` ## Backup and Recovery ### Backup Strategy #### 1. Data Backup ```bash # Daily backup script #!/bin/bash DATE=$(date +%Y%m%d) tar -czf backup/data_$DATE.tar.gz data/ tar -czf backup/results_$DATE.tar.gz results/ tar -czf backup/models_$DATE.tar.gz results/models/ # Keep last 30 days find backup/ -name "*.tar.gz" -mtime +30 -delete ``` #### 2. Configuration Backup ```bash # Backup configuration cp .env backup/env_$DATE cp requirements-pinned.txt backup/ git archive --format=tar.gz --output=backup/code_$DATE.tar.gz HEAD ``` ### Recovery Testing ```bash # Test recovery procedure monthly python scripts/run_tests.py --deployment python src/health_monitor.py --check-all ``` ## Security Checklist ### Pre-Production Security Audit - [ ] No hardcoded credentials in code - [ ] All secrets stored in secure vaults - [ ] Network access properly restricted - [ ] SSL/TLS encryption enabled - [ ] Regular security updates scheduled - [ ] Access logs monitored - [ ] Backup encryption enabled - [ ] Incident response plan documented ### Security Monitoring ```bash # Regular security checks python scripts/security_cleanup.py python src/health_monitor.py --check-all # Check for exposed credentials grep -r "sk-" . --exclude-dir=venv --exclude-dir=.git grep -r "api_key\s*=" . --exclude-dir=venv --exclude-dir=.git ``` ## Support and Maintenance ### Regular Maintenance Tasks #### Daily - [ ] Check health endpoints - [ ] Review error logs - [ ] Monitor resource usage #### Weekly - [ ] Run full test suite - [ ] Update model performance metrics - [ ] Review and rotate logs #### Monthly - [ ] Security audit - [ ] Dependency updates - [ ] Backup testing - [ ] Performance review ### Getting Help 1. **Check Documentation**: Review this guide and README.md 2. **Run Diagnostics**: Use `python scripts/run_tests.py --deployment` 3. **Check Health**: Use `python src/health_monitor.py --check-all` 4. **Review Logs**: Check `logs/` directory for error details 5. **Validate Configuration**: Run `python src/config_validator.py` ### Emergency Contacts - **System Administrator**: [Your contact info] - **Development Team**: [Team contact info] - **Cloud Provider Support**: [Support channels] --- ## Conclusion This deployment guide provides comprehensive instructions for deploying the Macro Sentiment Trading Pipeline in production. The system includes automated testing, health monitoring, and deployment validation tools to ensure reliable operation. For additional support or questions, refer to the troubleshooting section or contact the development team. 